{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dispkernel import dispKernel\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = np.loadtxt('traindata.csv', delimiter=',')\n",
    "trainlabel = np.loadtxt('trainlabel.csv', delimiter=',')\n",
    "validdata = np.loadtxt('validdata.csv', delimiter=',')\n",
    "validlabel = np.loadtxt('validlabel.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation and Loss Functions\n",
    "def linear(X, with_grad=True):\n",
    "    if with_grad:\n",
    "        dZ = np.ones_like(X)\n",
    "        return X, dZ\n",
    "    return X, None\n",
    "\n",
    "def sigmoid(X, with_grad=True):\n",
    "    Z = 1 / (1 + np.exp(-X))\n",
    "    if with_grad:\n",
    "        dZ = np.exp(-X) / (1 + np.exp(-X)) ** 2\n",
    "        return Z, dZ\n",
    "    return Z, None\n",
    "\n",
    "def ReLU(X, with_grad=True):\n",
    "    Z = np.max(X, 0)\n",
    "    if with_grad:\n",
    "        dZ = np.where(X > 0, 1, 0)\n",
    "        return Z, dZ\n",
    "    return Z, None\n",
    "\n",
    "def mse_loss(Z, label, with_grad=True):\n",
    "    L = np.sum((Z - label) ** 2)\n",
    "    if with_grad:\n",
    "        dLdZ = 2 * (Z - label)\n",
    "        return L, dLdZ\n",
    "    return L, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Neuron Classifier, with selectable activation function\n",
    "class Model:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.random_sample(9)\n",
    "        self.bias = np.random.random_sample()\n",
    "\n",
    "    def forward(self, X, with_grad=True):\n",
    "        Y = self.weights.dot(X) + self.bias\n",
    "        Z, dZ = self.activation(Y, with_grad)\n",
    "        if with_grad:\n",
    "            dYdW = X\n",
    "            dYdb = 1\n",
    "            dZdW = dZ * dYdW\n",
    "            dZdb = dZ * dYdb\n",
    "            return Z, dZdW, dZdb  \n",
    "        return Z, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(traindata, trainlabel, validdata, validlabel, activation, lr, epochs, debug=False):\n",
    "    neuron = Model(activation)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        Z, dZdW, dZdb = neuron.forward(traindata.T, with_grad=True)\n",
    "        L, dLdZ = mse_loss(Z, trainlabel, with_grad=True)\n",
    "        train_loss.append(L)\n",
    "        train_acc.append(((Z >= 0.5) == trainlabel).sum() / trainlabel.size)\n",
    "\n",
    "        dLdW = np.sum(dLdZ * dZdW, axis=1)\n",
    "        dLdb = np.sum(dLdZ * dZdb, axis=0)\n",
    "\n",
    "        neuron.weights -= dLdW * lr\n",
    "        neuron.bias -= dLdb * lr\n",
    "\n",
    "        val_out, _, _ = neuron.forward(validdata.T, with_grad=False)\n",
    "        val_loss.append(mse_loss(val_out, validlabel, with_grad=False)[0])\n",
    "        val_acc.append(((val_out >= 0.5) == validlabel).sum() / validlabel.size)\n",
    "\n",
    "        if debug:\n",
    "            print('epoch {}, train loss {:2f} acc {:2f} validation loss {:2f} acc {:2f}'.format(i, L, train_acc[-1], val_loss[-1], val_acc[-1]))\n",
    "    \n",
    "    return neuron, train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "def plot_history(hyperparameters, train_loss, train_acc, val_loss, val_acc):\n",
    "    plt.suptitle('Single Neuron Classifier, {} activation function, {} epochs, learning rate = {}'.format(\n",
    "        hyperparameters['activation'].__name__,\n",
    "        hyperparameters['epochs'],\n",
    "        hyperparameters['lr']\n",
    "    ))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.plot(train_loss, label='training')\n",
    "    plt.plot(val_loss, label='validation')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.plot(train_acc, label='training')\n",
    "    plt.plot(val_acc, label='validation')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the hyperparameter cells here are run the training loop for one of the scenarios described in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR too low\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.0000001,\n",
    "    'epochs': 1000,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR too high\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.002,\n",
    "    'epochs': 100,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR good\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 100,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.0005,\n",
    "    'epochs': 100,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "hyperparameters = {\n",
    "    'activation': ReLU,\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 100,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "hyperparameters = {\n",
    "    'activation': sigmoid,\n",
    "    'lr': 0.005,\n",
    "    'epochs': 100,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch test, 5, 10, 25, 100\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 5,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test, 0.05, 0.005, 0.001, 0.0005, 0.00005, 0.000005\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.05,\n",
    "    'epochs': 25,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function test\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 100,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed test\n",
    "hyperparameters = {\n",
    "    'activation': linear,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 25,\n",
    "    'seed': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best\n",
    "hyperparameters = {\n",
    "    'activation': sigmoid,\n",
    "    'lr': 0.1,\n",
    "    'epochs': 10,\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs the training loop with the defined hyperparameters, and plots the training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(hyperparameters['seed'])\n",
    "neuron, train_loss, train_acc, val_loss, val_acc = train(\n",
    "    traindata, trainlabel, validdata, validlabel,\n",
    "    hyperparameters['activation'], hyperparameters['lr'], hyperparameters['epochs'])\n",
    "plot_history(hyperparameters, train_loss, train_acc, val_loss, val_acc)\n",
    "print(train_acc[-1], val_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispKernel(neuron.weights, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neuron.weights, neuron.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
