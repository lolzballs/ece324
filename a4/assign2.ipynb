{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchsummary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn.metrics\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distribution(path):\n",
    "    dataset = torchvision.datasets.ImageFolder(path, transform=torchvision.transforms.ToTensor())\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "    images, _ = iter(dataloader).next()\n",
    "    images = images.view(-1,3,56*56).transpose(1,0).reshape(3,-1)\n",
    "    return images.mean(1), images.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(folder, num_images):\n",
    "    dataset = torchvision.datasets.ImageFolder(folder, transform=torchvision.transforms.ToTensor())\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=num_images)\n",
    "    images, labels = iter(dataloader).next()\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    images_np = images.numpy()\n",
    "    plt.imshow(np.transpose(images_np, (1, 2, 0)))\n",
    "    plt.xlabel([dataset.classes[label] for label in labels])\n",
    "    plt.show()\n",
    "\n",
    "display_sample_images('personal', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a basic model for showing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 4, 3)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 11 * 11, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 11 * 11)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to plot training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(train_loss, train_acc, val_loss=None, val_acc=None):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(train_loss, label='train')\n",
    "    if val_loss is not None:\n",
    "        plt.plot(val_loss, label='validation')\n",
    "        plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(train_acc, label='train')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    if val_acc is not None:\n",
    "        plt.plot(val_acc, label='validation')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define MSELoss training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mse(model, val_loader):\n",
    "    criterion = nn.MSELoss()\n",
    "    steps = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        onehot = torch.zeros(images.size(0), 10).scatter_(1, labels.unsqueeze(1), 1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, onehot)\n",
    "\n",
    "        avg_loss += float(loss)\n",
    "        avg_acc += float((pred.argmax(dim=1) == labels).sum().item()) / images.size(0)\n",
    "        steps += 1\n",
    "        \n",
    "        del loss, pred, onehot\n",
    "    return avg_loss / steps, avg_acc / steps\n",
    "\n",
    "def train_mse(Model, train_set, batch_size, lr, epochs, val_set=None):\n",
    "    model = Model().to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "    if val_set is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, shuffle=True, batch_size=batch_size)\n",
    "    optim = torch.optim.SGD(model.parameters(), lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    val_acc = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        steps = 0\n",
    "        avg_acc = 0\n",
    "        avg_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            onehot = torch.zeros(images.size(0), 10).scatter_(1, labels.unsqueeze(1), 1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, onehot)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            avg_loss += float(loss)\n",
    "            avg_acc += float((pred.argmax(dim=1) == labels).sum().item()) / images.size(0)\n",
    "            steps += 1\n",
    "            \n",
    "            del loss, pred, onehot\n",
    "    \n",
    "        avg_loss /= steps\n",
    "        avg_acc /= steps\n",
    "        \n",
    "        train_loss.append(avg_loss)\n",
    "        train_acc.append(avg_acc)\n",
    "        \n",
    "        if val_set is not None:\n",
    "            with torch.no_grad():\n",
    "                validate = validate_mse(model, val_loader)\n",
    "            val_loss.append(validate[0])\n",
    "            val_acc.append(validate[1])\n",
    "            print(\"{}\\ttrain loss {:.4f}\\tacc {:.4f}\\tval loss {:.4f}\\t val acc {:.4f}\".format(\n",
    "                i+1, avg_loss, avg_acc, validate[0], validate[1]\n",
    "            ))\n",
    "        else:\n",
    "            print(\"{}, loss {:.4f}, acc {:.4f}\".format(i+1, avg_loss, avg_acc))\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, train_acc, train_loss, val_acc, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collected Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(*calculate_distribution('personal')),\n",
    "])\n",
    "\n",
    "personal = torchvision.datasets.ImageFolder('personal', transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "tic = time.monotonic()\n",
    "model, train_acc, train_loss, _, _ = train_mse(BasicModel, personal, 10, 0.05, 100)\n",
    "toc = time.monotonic()\n",
    "print(f\"{toc - tic} seconds elapsed\")\n",
    "torchsummary.summary(model, (3, 56, 56))\n",
    "plot_history(train_loss, train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.ImageFolder('train', transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(*calculate_distribution('train')),\n",
    "]))\n",
    "val_set = torchvision.datasets.ImageFolder('val', transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(*calculate_distribution('val')),\n",
    "]))\n",
    "calculate_distribution('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models\n",
    "Here we define 6 different models to train on for section 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Layers30Kernels(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv2Layers30Kernels, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 30, 3)\n",
    "        self.conv2 = nn.Conv2d(30, 30, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(30 * 12 * 12, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = x.view(-1, 30 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(Conv2Layers30Kernels(), (3, 56, 56), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Layers10Kernels(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv2Layers10Kernels, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(10 * 12 * 12, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(Conv2Layers10Kernels(), (3, 56, 56), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1Layers10Kernels(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1Layers10Kernels, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(10 * 27 * 27, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = x.view(-1, 10 * 27 * 27)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(Conv1Layers10Kernels(), (3, 56, 56), device='cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1Layers30Kernels(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1Layers30Kernels, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 30, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(30 * 27 * 27, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = x.view(-1, 30 * 27 * 27)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(Conv1Layers30Kernels(), (3, 56, 56), device='cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3Layers30Kernels(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3Layers30Kernels, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 30, 3)\n",
    "        self.conv2 = nn.Conv2d(30, 30, 3)\n",
    "        self.conv3 = nn.Conv2d(30, 30, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(30 * 5 * 5, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        x = x.view(-1, 30 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(Conv3Layers30Kernels(), (3, 56, 56), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3Layers10Kernels(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3Layers10Kernels, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, 3)\n",
    "        self.conv3 = nn.Conv2d(10, 10, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(10 * 5 * 5, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        x = x.view(-1, 10 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(Conv3Layers10Kernels(), (3, 56, 56), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with MSELoss\n",
    "This is the training code. We modify the model and learning rate and track the accuracy and loss throughout the training process. The execution time and max accuracy obtained are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Model': Conv3Layers10Kernels,\n",
    "    'lr': 0.1,\n",
    "    'batch_size': 32, # we fix this\n",
    "    'epochs': 1000 # keep this fixed so we can get comparable execution times\n",
    "}\n",
    "\n",
    "torch.manual_seed(100)\n",
    "tic = time.monotonic()\n",
    "model, train_acc, train_loss, val_acc, val_loss = train_mse(**hyperparameters, train_set=train_set, val_set=val_set)\n",
    "toc = time.monotonic()\n",
    "print(f\"seconds elapsed: {toc - tic}\")\n",
    "print(f\"max accuracy obatined: {np.max(val_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(train_loss, train_acc, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss\n",
    "Define new training and validation functions utilizing cross entropy loss rather than MSELoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ce(model, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    steps = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, labels)\n",
    "\n",
    "        avg_loss += float(loss)\n",
    "        avg_acc += float((pred.argmax(dim=1) == labels).sum().item()) / images.size(0)\n",
    "        steps += 1\n",
    "        \n",
    "        del loss, pred\n",
    "    return avg_loss / steps, avg_acc / steps\n",
    "\n",
    "def train_ce(Model, train_set, batch_size, lr, epochs, val_set=None):\n",
    "    model = Model().to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "    if val_set is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, shuffle=True, batch_size=batch_size)\n",
    "    optim = torch.optim.SGD(model.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    val_acc = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        steps = 0\n",
    "        avg_acc = 0\n",
    "        avg_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            avg_loss += float(loss)\n",
    "            avg_acc += float((pred.argmax(dim=1) == labels).sum().item()) / images.size(0)\n",
    "            steps += 1\n",
    "            \n",
    "            del loss, pred\n",
    "    \n",
    "        avg_loss /= steps\n",
    "        avg_acc /= steps\n",
    "        \n",
    "        train_loss.append(avg_loss)\n",
    "        train_acc.append(avg_acc)\n",
    "        \n",
    "        if val_set is not None:\n",
    "            with torch.no_grad():\n",
    "                validate = validate_mse(model, val_loader)\n",
    "            val_loss.append(validate[0])\n",
    "            val_acc.append(validate[1])\n",
    "            print(\"{}\\ttrain loss {:.4f}\\tacc {:.4f}\\tval loss {:.4f}\\t val acc {:.4f}\".format(\n",
    "                i+1, avg_loss, avg_acc, validate[0], validate[1]\n",
    "            ))\n",
    "        else:\n",
    "            print(\"{}, loss {:.4f}, acc {:.4f}\".format(i+1, avg_loss, avg_acc))\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, train_acc, train_loss, val_acc, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "We now implement Conv3Layers10Kernels with Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3Layers10KernelsBatch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3Layers10KernelsBatch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, 3)\n",
    "        self.conv3 = nn.Conv2d(10, 10, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.bn2 = nn.BatchNorm2d(10)\n",
    "        self.bn3 = nn.BatchNorm2d(10)\n",
    "        self.fc1 = nn.Linear(10 * 5 * 5, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.pool(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.pool(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.pool(self.conv3(x))));\n",
    "        x = x.view(-1, 10 * 5 * 5)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(Conv3Layers10KernelsBatch(), (3, 56, 56), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using MSELoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Model': Conv3Layers10KernelsBatch,\n",
    "    'lr': 0.1,\n",
    "    'batch_size': 32, # we fix this\n",
    "    'epochs': 1000 # keep this fixed so we can get comparable execution times\n",
    "}\n",
    "\n",
    "torch.manual_seed(100)\n",
    "tic = time.monotonic()\n",
    "model, train_acc, train_loss, val_acc, val_loss = train_mse(**hyperparameters, train_set=train_set, val_set=val_set)\n",
    "toc = time.monotonic()\n",
    "print(f\"seconds elapsed: {toc - tic}\")\n",
    "print(f\"max accuracy obatined: {np.max(val_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(train_loss, train_acc, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Model': Conv3Layers10Kernels, \n",
    "    'lr': 0.09,\n",
    "    'batch_size': 32, # we fix this\n",
    "    'epochs': 1000 # keep this fixed so we can get comparable execution times\n",
    "}\n",
    "\n",
    "torch.manual_seed(100)\n",
    "tic = time.monotonic()\n",
    "model, train_acc, train_loss, val_acc, val_loss = train_ce(**hyperparameters, train_set=train_set, val_set=val_set)\n",
    "toc = time.monotonic()\n",
    "print(f\"seconds elapsed: {toc - tic}\")\n",
    "print(f\"max accuracy obatined: {np.max(val_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(train_loss, train_acc, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Functions\n",
    "\n",
    "Define a couple functions to assist with plotting the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_labels(model, val_set):\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=len(val_set))\n",
    "    data, labels = iter(val_loader).next()\n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels_gpu = labels.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(data)\n",
    "    return preds.argmax(dim=1).cpu().numpy(), labels.cpu().numpy()\n",
    "\n",
    "def plot_confusion_matrix(model, val_set):\n",
    "    preds, labels = evaluate_all_labels(model, val_set)\n",
    "    cm = sklearn.metrics.confusion_matrix(labels, preds)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_set.classes)\n",
    "    return disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.pool(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.pool(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.pool(self.conv3(x))))\n",
    "        x = F.relu(self.bn4(self.pool(self.conv4(x))))\n",
    "        x = x.view(-1, 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(LargeModel(), (3, 56, 56), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Model': LargeModel, \n",
    "    'lr': 0.005,\n",
    "    'batch_size': 24,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "torch.manual_seed(100)\n",
    "tic = time.monotonic()\n",
    "model, train_acc, train_loss, val_acc, val_loss = train_ce(**hyperparameters, train_set=train_set, val_set=val_set)\n",
    "toc = time.monotonic()\n",
    "print(f\"seconds elapsed: {toc - tic}\")\n",
    "print(f\"max accuracy obatined: {np.max(val_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(train_loss, train_acc, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MyBest.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Small Model\n",
    "\n",
    "Define and train a model with less than 5000 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 7, 3)\n",
    "        self.conv2 = nn.Conv2d(7, 14, 3)\n",
    "        self.conv3 = nn.Conv2d(14, 10, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(10 * 5 * 5, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        #x = F.relu(self.pool(self.conv4(x)))\n",
    "        x = x.view(-1, 10 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "torchsummary.summary(SmallModel(), (3, 56, 56), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'Model': SmallModel, \n",
    "    'lr': 0.01,\n",
    "    'batch_size': 24,\n",
    "    'epochs': 100 # keep this fixed so we can get comparable execution times\n",
    "}\n",
    "\n",
    "torch.manual_seed(100)\n",
    "tic = time.monotonic()\n",
    "model, train_acc, train_loss, val_acc, val_loss = train_ce(**hyperparameters, train_set=train_set, val_set=val_set)\n",
    "toc = time.monotonic()\n",
    "print(f\"seconds elapsed: {toc - tic}\")\n",
    "print(f\"max accuracy obatined: {np.max(val_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(train_loss, train_acc, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MyBestSmall.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
